import os
import json
from PIL import Image
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from tqdm import tqdm

# è®¾ç½®å¯è§GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "5,6,7,8,9"
torch.set_float32_matmul_precision('high')

def load_image(image_path):
    """åŠ è½½æœ¬åœ°å›¾ç‰‡å¹¶è¿”å›PIL Imageå¯¹è±¡ï¼ˆé€‚é…ç¤ºä¾‹ä¸ä¸»æ•°æ®çš„å›¾ç‰‡è·¯å¾„ï¼‰"""
    try:
        if os.path.exists(image_path):
            return Image.open(image_path).convert("RGB")
        else:
            print(f"å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨: {image_path}")
            return None
    except Exception as e:
        print(f"åŠ è½½å›¾ç‰‡å¤±è´¥ {image_path}: {str(e)}")
        return None

def load_system_prompt(file_path):
    """ä»æ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤ºå†…å®¹"""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()

def load_examples(example_json_path, image_dir):
    """åŠ è½½ç¤ºä¾‹JSONæ–‡ä»¶ï¼ˆä¸¥æ ¼åŒ¹é…ç¤ºä¾‹å­—æ®µæ ¼å¼ï¼‰ï¼Œè¿”å›å‰3æ¡æœ‰æ•ˆç¤ºä¾‹"""
    try:
        with open(example_json_path, "r", encoding="utf-8") as f:
            raw_examples = json.load(f)
        
        processed_examples = []
        # éå†å¹¶ç­›é€‰æœ‰æ•ˆç¤ºä¾‹ï¼ˆç¡®ä¿åŒ…å«å…³é”®å­—æ®µï¼‰
        for idx, example in enumerate(raw_examples):
            if len(processed_examples) >= 3:  # ä»…ä¿ç•™å‰3æ¡ç¤ºä¾‹
                break
            
            # æ ¡éªŒç¤ºä¾‹çš„å…³é”®å­—æ®µï¼ˆåŒ¹é…ç”¨æˆ·æä¾›çš„ç¤ºä¾‹æ ¼å¼ï¼‰
            required_fields = ["image_relative_path", "query", "thinking", "response", "eval_result"]
            if not all(field in example for field in required_fields):
                print(f"è·³è¿‡æ— æ•ˆç¤ºä¾‹ï¼ˆç¼ºå¤±å…³é”®å­—æ®µï¼‰: ç¬¬{idx+1}æ¡")
                continue
            
            # å¤„ç†ç¤ºä¾‹å›¾ç‰‡ï¼ˆä½¿ç”¨image_relative_pathæ„å»ºå®Œæ•´è·¯å¾„ï¼‰
            example_image_path = os.path.join(image_dir, example["image_relative_path"])
            example_image = load_image(example_image_path)
            
            # å°è£…ç¤ºä¾‹æ•°æ®ï¼ˆä¸åŸå§‹æ ¼å¼ä¸€è‡´ï¼Œä¾¿äºåç»­æ ¼å¼åŒ–ï¼‰
            processed_examples.append({
                "image_relative_path": example["image_relative_path"],
                "image": example_image,  # åŠ è½½åçš„å›¾ç‰‡å¯¹è±¡
                "query": example["query"],
                "thinking": example["thinking"],
                "response": example["response"],
                "eval_result": example["eval_result"]
            })
        
        if len(processed_examples) < 3:
            print(f"è­¦å‘Šï¼šä»…åŠ è½½åˆ°{len(processed_examples)}æ¡æœ‰æ•ˆç¤ºä¾‹ï¼ˆæœŸæœ›3æ¡ï¼‰")
        return processed_examples
    except Exception as e:
        print(f"åŠ è½½ç¤ºä¾‹JSONå¤±è´¥: {str(e)}")
        return []

def format_examples_as_dialog(examples):
    """å°†ç¤ºä¾‹æ ¼å¼åŒ–ä¸ºâ€œç”¨æˆ·-åŠ©æ‰‹â€å¯¹è¯å½¢å¼ï¼ˆç¬¦åˆQwenæ¨¡å‹å¯¹è¯é€»è¾‘ï¼‰"""
    dialog_parts = [
        "Below are 3 reference examples, including the query, thinking process, final response, and evaluation result. Please strictly imitate the output style, ethical stance, and format of the examples for processing:\n"
    ]
    
    for idx, example in enumerate(examples, 1):
        # ç¤ºä¾‹æ ‡é¢˜ï¼ˆæ ‡æ³¨ç±»åˆ«å’Œå›¾ç‰‡ä¿¡æ¯ï¼‰
        dialog_parts.append(f"\n=== example{idx} ===")
        dialog_parts.append(f"image_pathï¼š{example['image_relative_path']}")
        dialog_parts.append(f"1.Queryï¼š{example['query']}")
        dialog_parts.append(f"2.model_thinkingï¼š{example['thinking']}")
        dialog_parts.append(f"3.model_responseï¼š{example['response']}")
        dialog_parts.append(f"4.eval_resultsï¼š{example['eval_result']}")
    
    # å¼•å¯¼æ¨¡å‹å¤„ç†å½“å‰ä»»åŠ¡
    dialog_parts.append("\n--- Please process the following current task based on the above examples. ---")
    return "\n".join(dialog_parts)

def process_json_data(input_json, output_json, system_prompt, image_dir, 
                     model, processor, examples):
    """å¤„ç†ä¸»æ•°æ®ï¼ˆæ•´åˆç¤ºä¾‹å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰"""
    # è¯»å–ä¸»æ•°æ®JSON
    with open(input_json, "r", encoding="utf-8") as f:
        data_items = json.load(f)
    
    results = []
    device = model.device
    # é¢„å¤„ç†ç¤ºä¾‹ä¸ºå¯¹è¯æ ¼å¼ï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
    example_dialog = format_examples_as_dialog(examples)
    
    # å¸¦è¿›åº¦æ¡å¤„ç†æ¯ä¸ªä¸»æ•°æ®æ¡ç›®
    for item in tqdm(data_items, desc="Qwen2.5-VL å¤„ç†è¿›åº¦", unit="æ¡"):
        try:
            # 1. å¤„ç†å½“å‰æ¡ç›®çš„å›¾ç‰‡
            item_image_path = os.path.join(image_dir, item.get("image_name", ""))
            item_image = load_image(item_image_path) if item.get("image_name") else None
            
            # 2. æ„å»ºå®Œæ•´å¯¹è¯ï¼ˆç³»ç»Ÿæç¤º + ç¤ºä¾‹ä¸Šä¸‹æ–‡ + å½“å‰ä»»åŠ¡ï¼‰
            messages = [
                # ç³»ç»Ÿæç¤ºï¼ˆä¼˜å…ˆå®šä¹‰æ•´ä½“è§„åˆ™ï¼‰
                {
                    "role": "system",
                    "content": [{"type": "text", "text": system_prompt}]
                },
                # ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«ç¤ºä¾‹ä¸Šä¸‹æ–‡ + å½“å‰ä»»åŠ¡çš„å›¾ç‰‡/æ–‡æœ¬ï¼‰
                {
                    "role": "user",
                    "content": []
                }
            ]
            
            # 2.1 æ·»åŠ ç¤ºä¾‹å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä½œä¸ºå½“å‰ä»»åŠ¡çš„å‰ç½®å‚è€ƒï¼‰
            messages[1]["content"].append({"type": "text", "text": example_dialog})
            
            # 2.2 æ·»åŠ å½“å‰ä»»åŠ¡çš„å›¾ç‰‡ï¼ˆè‹¥å­˜åœ¨ï¼‰
            if item_image:
                messages[1]["content"].append({
                    "type": "image", 
                    "image": item_image,
                    "caption": f"å½“å‰ä»»åŠ¡å›¾ç‰‡ï¼š{item.get('image_name')}"  # æ ‡æ³¨å›¾ç‰‡ä¿¡æ¯
                })
            else:
                messages[1]["content"].append({
                    "type": "text", 
                    "text": "å½“å‰ä»»åŠ¡ç¼ºå°‘å›¾ç‰‡æˆ–å›¾ç‰‡æ— æ³•è®¿é—®"
                })
            
            # 2.3 æ·»åŠ å½“å‰ä»»åŠ¡çš„æ–‡æœ¬å†…å®¹ï¼ˆåŒ¹é…ç¤ºä¾‹çš„queryæ ¼å¼ï¼‰
            item_text = (
                f"Current Task Query:{item.get('question','æ— æé—®')}\n"
                f"Current Task Thinking:{item.get('reasoning', 'æ— æ€è€ƒè¿‡ç¨‹')}\n"
                f"Current Task Cleaned Response:{item.get('response', 'æ— åŸå§‹å“åº”')}"
            )
            messages[1]["content"].append({"type": "text", "text": item_text})
            
            # 3. Qwenæ¨¡å‹è¾“å…¥å¤„ç†ï¼ˆå¤šæ¨¡æ€æ ¼å¼é€‚é…ï¼‰
            # 3.1 ç”Ÿæˆå¯¹è¯æ¨¡æ¿æ–‡æœ¬
            chat_text = processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True  # è‡ªåŠ¨æ·»åŠ ç”Ÿæˆæç¤º
            )
            # 3.2 å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆå›¾ç‰‡ï¼‰
            image_inputs, video_inputs = process_vision_info(messages)
            # 3.3 ç¼–ç ä¸ºæ¨¡å‹å¯æ¥å—çš„å¼ é‡
            inputs = processor(
                text=[chat_text],
                images=image_inputs,
                videos=video_inputs,
                padding="longest",  # é€‚é…å¤šæ¨¡æ€è¾“å…¥é•¿åº¦
                return_tensors="pt",
                truncation=True,  # é¿å…è¾“å…¥è¿‡é•¿ï¼ˆQwenæœ‰æœ€å¤§é•¿åº¦é™åˆ¶ï¼‰
                max_length=32768  # åŸºäºQwen2.5-VL-32Bçš„é»˜è®¤æœ€å¤§é•¿åº¦
            ).to(device)
            
            # 4. æ¨¡å‹ç”Ÿæˆï¼ˆç¡®å®šæ€§ç”Ÿæˆï¼Œé¿å…æ— æ•ˆå‚æ•°ï¼‰
            with torch.inference_mode():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=2000,  # è¶³å¤Ÿå®¹çº³æ€è€ƒ+å“åº”+è¯„ä¼°ç»“æœ
                    do_sample=False,  # ç¦ç”¨é‡‡æ ·ï¼Œç¡®ä¿è¾“å‡ºç¨³å®šï¼ˆåŒ¹é…ç¤ºä¾‹é£æ ¼ï¼‰
                    temperature=None,  # ç¡®å®šæ€§ç”Ÿæˆæ—¶æ— éœ€æ¸©åº¦å‚æ•°
                    top_p=None,  # ç§»é™¤æ— æ•ˆé‡‡æ ·å‚æ•°ï¼Œé¿å…æŠ¥é”™
                    top_k=None
                )
            
            # 5. ç»“æœè§£ç ä¸ä¿®å‰ªï¼ˆä»…ä¿ç•™æ–°ç”Ÿæˆéƒ¨åˆ†ï¼‰
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            # è§£ç æ—¶ä¿ç•™åŸå§‹æ ¼å¼ï¼ˆä¸æ¸…ç†ç©ºæ ¼ï¼ŒåŒ¹é…ç¤ºä¾‹çš„æ¢è¡Œç»“æ„ï¼‰
            eval_result = processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            # 6. ä¿å­˜ç»“æœï¼ˆä¸ä¸»æ•°æ®æ ¼å¼ä¸€è‡´ï¼Œæ–°å¢ç¤ºä¾‹å‚è€ƒæ ‡è®°ï¼‰
            result_entry = {
                "image_name": item.get("image_name", "æ— å›¾ç‰‡"),
                "query": item.get("question", ""),
                "thinking": item.get("reasoning", ""),
                "response": item.get("response", ""),
                "eval_result": eval_result
            }
            if not item_image:
                result_entry["note"] += " | ç¼ºå°‘å½“å‰ä»»åŠ¡å›¾ç‰‡"
            
            results.append(result_entry)
        
        except Exception as e:
            # é”™è¯¯å¤„ç†ï¼ˆä¿ç•™é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºåç»­æ’æŸ¥ï¼‰
            error_msg = f"å¤„ç†æ¡ç›®[{item.get('image_name', 'æœªçŸ¥')}]å¤±è´¥: {str(e)}"
            print(f"\n{error_msg}")
            results.append({
                "image_name": item.get("image_name", "æœªçŸ¥å›¾ç‰‡"),
                "error": error_msg,
                "note": "å¤„ç†å¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰"
            })
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\næ‰€æœ‰å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{output_json}")
    print(f"æˆåŠŸå¤„ç†ï¼š{len([r for r in results if 'error' not in r])}æ¡ | å¤±è´¥ï¼š{len([r for r in results if 'error' in r])}æ¡")

def main():
    # -------------------------- é…ç½®å‚æ•° --------------------------
    CONFIG = {
        "system_prompt_file": "system_prompt_1.txt",  # ç³»ç»Ÿæç¤ºæ–‡ä»¶
        "input_json_file": "MMVet_Kimi.json",  # ä¸»æ•°æ®æ–‡ä»¶
        "output_json_file": "Qwen2.5-VL_eval_Kimi.json",  # è¾“å‡ºæ–‡ä»¶
        "image_directory": "./images",  # å›¾ç‰‡æ ¹ç›®å½•
        "example_json_file": "icl_example.json",  # 3æ¡ç¤ºä¾‹çš„JSONæ–‡ä»¶è·¯å¾„
        "model_id": "Qwen/Qwen2.5-VL-32B-Instruct"  # Qwenæ¨¡å‹ID
    }
    # --------------------------------------------------------------
    
    # 1. åŠ è½½ç³»ç»Ÿæç¤º
    try:
        system_prompt = load_system_prompt(CONFIG["system_prompt_file"])
        print("âœ… ç³»ç»Ÿæç¤ºåŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ åŠ è½½ç³»ç»Ÿæç¤ºå¤±è´¥: {e}")
        return
    
    # 2. åŠ è½½3æ¡å‚è€ƒç¤ºä¾‹
    examples = load_examples(CONFIG["example_json_file"], CONFIG["image_directory"])
    if not examples:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆç¤ºä¾‹ï¼Œæ— æ³•è¿›è¡Œç¤ºä¾‹å¼•å¯¼ï¼Œç¨‹åºé€€å‡º")
        return
    print(f"âœ… åŠ è½½åˆ°{len(examples)}æ¡æœ‰æ•ˆå‚è€ƒç¤ºä¾‹")
    
    # 3. åŠ è½½Qwenæ¨¡å‹å’Œå¤„ç†å™¨
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{CONFIG['model_id']}ï¼ˆè¯·è€å¿ƒç­‰å¾…ï¼‰...")
    try:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            CONFIG["model_id"],
            torch_dtype="auto",  # è‡ªåŠ¨åŒ¹é…GPUæ”¯æŒçš„ç²¾åº¦ï¼ˆå¦‚bfloat16ï¼‰
            device_map="auto",  # è‡ªåŠ¨åˆ†é…å¤šGPUï¼ˆé€‚é…6-9å·GPUï¼‰
            low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜å ç”¨ï¼Œé¿å…OOM
            trust_remote_code=True  # åŠ è½½Qwençš„è‡ªå®šä¹‰ä»£ç ï¼ˆå¤šæ¨¡æ€å¤„ç†éœ€å¼€å¯ï¼‰
        ).eval()  # åˆ‡æ¢ä¸ºæ¨ç†æ¨¡å¼
        processor = AutoProcessor.from_pretrained(
            CONFIG["model_id"],
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä¸»è®¾å¤‡ï¼š{model.device}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # 4. å¤„ç†ä¸»æ•°æ®ï¼ˆæ•´åˆç¤ºä¾‹ä¸Šä¸‹æ–‡ï¼‰
    print("\nğŸš€ å¼€å§‹å¤„ç†ä¸»æ•°æ®...")
    process_json_data(
        input_json=CONFIG["input_json_file"],
        output_json=CONFIG["output_json_file"],
        system_prompt=system_prompt,
        image_dir=CONFIG["image_directory"],
        model=model,
        processor=processor,
        examples=examples
    )

if __name__ == "__main__":
    main()