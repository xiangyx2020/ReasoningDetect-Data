import os
import json
from PIL import Image
import torch
from datetime import datetime, timedelta
import pathlib
from tqdm import tqdm

from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from huggingface_hub import hf_hub_download
from transformers import Mistral3ForConditionalGeneration

# è®¾ç½®å¯è§GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4"
# å¯ç”¨TensorFloat32æå‡æ€§èƒ½ï¼ˆé’ˆå¯¹æ”¯æŒçš„GPUï¼‰
torch.set_float32_matmul_precision('high')

def load_image(image_path):
    """åŠ è½½æœ¬åœ°å›¾ç‰‡å¹¶è¿”å›PIL Imageå¯¹è±¡"""
    try:
        if os.path.exists(image_path):
            return Image.open(image_path).convert("RGB")
        else:
            print(f"å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨: {image_path}")
            return None
    except Exception as e:
        print(f"åŠ è½½å›¾ç‰‡å¤±è´¥ {image_path}: {str(e)}")
        return None

def get_file_url(path):
    """å°†æœ¬åœ°æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºfile://æ ¼å¼çš„URL"""
    absolute_path = os.path.abspath(path)
    return pathlib.Path(absolute_path).as_uri()

def load_system_prompt(file_path):
    """ä»æ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤ºå†…å®¹"""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()

def load_examples(example_json_path, image_dir):
    """åŠ è½½ç¤ºä¾‹JSONæ–‡ä»¶ï¼Œè¿”å›å‰3æ¡æœ‰æ•ˆç¤ºä¾‹"""
    try:
        with open(example_json_path, "r", encoding="utf-8") as f:
            raw_examples = json.load(f)
        
        processed_examples = []
        # éå†å¹¶ç­›é€‰æœ‰æ•ˆç¤ºä¾‹
        for idx, example in enumerate(raw_examples):
            if len(processed_examples) >= 3:  # ä»…ä¿ç•™å‰3æ¡ç¤ºä¾‹
                break
            
            # æ ¡éªŒç¤ºä¾‹çš„å…³é”®å­—æ®µ
            required_fields = ["image_relative_path", "query", "thinking", "response", "eval_result"]
            if not all(field in example for field in required_fields):
                print(f"è·³è¿‡æ— æ•ˆç¤ºä¾‹ï¼ˆç¼ºå¤±å…³é”®å­—æ®µï¼‰: ç¬¬{idx+1}æ¡")
                continue
            
            # å¤„ç†ç¤ºä¾‹å›¾ç‰‡
            example_image_path = os.path.join(image_dir, example["image_relative_path"])
            example_image = load_image(example_image_path)
            
            # å°è£…ç¤ºä¾‹æ•°æ®
            processed_examples.append({
                "image_relative_path": example["image_relative_path"],
                "image": example_image,
                "query": example["query"],
                "thinking": example["thinking"],
                "response": example["response"],
                "eval_result": example["eval_result"]
            })
        
        if len(processed_examples) < 3:
            print(f"è­¦å‘Šï¼šä»…åŠ è½½åˆ°{len(processed_examples)}æ¡æœ‰æ•ˆç¤ºä¾‹ï¼ˆæœŸæœ›3æ¡ï¼‰")
        return processed_examples
    except Exception as e:
        print(f"åŠ è½½ç¤ºä¾‹JSONå¤±è´¥: {str(e)}")
        return []

def format_examples_as_dialog(examples):
    """å°†ç¤ºä¾‹æ ¼å¼åŒ–ä¸ºå¯¹è¯å½¢å¼"""
    dialog_parts = [
        "Below are 3 reference examples, including the query, thinking process, final response, and evaluation result. Please strictly imitate the output style, ethical stance, and format of the examples for processing:\n"
    ]
    
    for idx, example in enumerate(examples, 1):
        dialog_parts.append(f"\n=== example{idx} ===")
        dialog_parts.append(f"image_pathï¼š{example['image_relative_path']}")
        dialog_parts.append(f"1.Queryï¼š{example['query']}")
        dialog_parts.append(f"2.model_thinkingï¼š{example['thinking']}")
        dialog_parts.append(f"3.model_responseï¼š{example['response']}")
        dialog_parts.append(f"4.eval_resultsï¼š{example['eval_result']}")
    
    dialog_parts.append("\n--- Please process the following current task based on the above examples. ---")
    return "\n".join(dialog_parts)

def process_json_data(input_json, output_json, system_prompt, image_dir, model, tokenizer, examples):
    """å¤„ç†JSONæ•°æ®ï¼Œè°ƒç”¨Mistralæ¨¡å‹ç”Ÿæˆç»“æœå¹¶ä¿å­˜ï¼ˆå¸¦ç¤ºä¾‹ä¸Šä¸‹æ–‡ï¼‰"""
    # è¯»å–è¾“å…¥JSONæ–‡ä»¶
    with open(input_json, "r", encoding="utf-8") as f:
        data_items = json.load(f) 
    
    results = []
    device = model.device  # è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡
    example_dialog = format_examples_as_dialog(examples)  # é¢„å¤„ç†ç¤ºä¾‹ä¸ºå¯¹è¯æ ¼å¼
    
    # å¸¦è¿›åº¦æ¡å¤„ç†æ¯ä¸ªæ¡ç›®
    for item in tqdm(data_items, desc="Mistral-3.2 å¤„ç†è¿›åº¦", unit="æ¡"):
        try:
            # è·å–å›¾ç‰‡åç§°å¹¶æ„å»ºå®Œæ•´è·¯å¾„
            image_name = item.get("image_used")
            image_path = os.path.join(image_dir, image_name) if image_name else None
            
            # åŠ è½½å›¾ç‰‡
            image = None
            if image_name and image_path and os.path.exists(image_path):
                image = load_image(image_path)
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": []
                }
            ]
            
            # æ·»åŠ ç¤ºä¾‹å¯¹è¯ä¸Šä¸‹æ–‡
            messages[1]["content"].append({"type": "text", "text": example_dialog})
            
            # æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if image:
                # ä¿®å¤å›¾ç‰‡URLæ ¼å¼
                image_url = get_file_url(image_path)
                messages[1]["content"].append({"type": "image_url", "image_url": {"url": image_url}})
            else:
                missing_note = "æ³¨æ„ï¼šè¯¥æŸ¥è¯¢ç¼ºå°‘ç›¸å…³å›¾ç‰‡æˆ–å›¾ç‰‡æ— æ³•è®¿é—®ã€‚"
                print(f"ç¼ºå°‘å›¾ç‰‡æˆ–å›¾ç‰‡æ— æ³•è®¿é—®: {image_name or 'æœªæä¾›å›¾ç‰‡åç§°'}")
                messages[1]["content"].append({"type": "text", "text": missing_note})
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹
            text_content = (
                f"Current Task Query:{item.get('Behavior', 'æ— æé—®')}\n"
                f"Current Task Thinking:{item.get('think', 'æ— æ€è€ƒè¿‡ç¨‹')}\n"
                f"Current Task Cleaned Response:{item.get('cleaned_response', 'æ— åŸå§‹å“åº”')}"
            )
            messages[1]["content"].append({"type": "text", "text": text_content})
            
            # å¤„ç†è¾“å…¥ - ä½¿ç”¨Mistralçš„æ–¹å¼ç¼–ç 
            tokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))
            
            input_ids = torch.tensor([tokenized.tokens]).to(device)
            attention_mask = torch.ones_like(input_ids).to(device)
            
            # å¤„ç†å›¾åƒè¾“å…¥
            pixel_values = None
            image_sizes = None
            if image:
                pixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0).to(device)
                image_sizes = torch.tensor([pixel_values.shape[-2:]]).to(device)
            
            # ç”Ÿæˆç»“æœ
            with torch.inference_mode():
                generation = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    pixel_values=pixel_values,
                    image_sizes=image_sizes,
                    max_new_tokens=1000,
                )[0]
            
            # è§£ç ç»“æœ - ç§»é™¤skip_special_tokenså‚æ•°ï¼Œå› ä¸ºMistralTokenizerä¸æ”¯æŒ
            eval_result = tokenizer.decode(generation[len(tokenized.tokens):])
            
            # å‡†å¤‡ç»“æœæ¡ç›®
            result_entry = {
                "image_name": image_name or "æ— å›¾ç‰‡åç§°",
                "BehaviorID": item.get("BehaviorID", ""),
                "query": item.get("Behavior", ""),
                "thinking": item.get("think", ""),
                "response": item.get("cleaned_response", ""),
                "eval_result": eval_result
            }
            
            if not image:
                result_entry["note"] += " | è¯¥æ¡ç›®å¤„ç†æ—¶ç¼ºå°‘å›¾ç‰‡æˆ–å›¾ç‰‡æ— æ³•è®¿é—®"
                
            results.append(result_entry)
            
        except Exception as e:
            error_msg = f"å¤„ç† {item.get('image_used', 'æœªçŸ¥æ¡ç›®')} æ—¶å‡ºé”™: {str(e)}"
            print(f"\n{error_msg}")
            results.append({
                "image_name": item.get("image_used", "æœªçŸ¥å›¾ç‰‡"),
                "error": error_msg,
                "note": "å¤„ç†å¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰"
            })
    
    # ä¿å­˜ç»“æœ
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    success_count = len([r for r in results if "error" not in r])
    fail_count = len(results) - success_count
    print(f"\næ‰€æœ‰å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {output_json}")
    print(f"æˆåŠŸå¤„ç†ï¼š{success_count}æ¡ | å¤±è´¥ï¼š{fail_count}æ¡")

def main():
    # é…ç½®æ–‡ä»¶è·¯å¾„
    CONFIG = {
        "system_prompt_file": "system_prompt_1.txt",
        "input_json_file": "S-Eval_3_GLM.json",
        "output_json_file": "Mistral-3.2_eval_GLM.json",
        "image_directory": "./selected_llava_images",
        "example_json_file": "icl_example.json",  # ç¤ºä¾‹æ–‡ä»¶è·¯å¾„
        "model_id": "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
    }
    
    # åŠ è½½ç³»ç»Ÿæç¤º
    try:
        system_prompt = load_system_prompt(CONFIG["system_prompt_file"])
        print("âœ… ç³»ç»Ÿæç¤ºåŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ åŠ è½½ç³»ç»Ÿæç¤ºå¤±è´¥: {str(e)}")
        return
    
    # åŠ è½½3æ¡å‚è€ƒç¤ºä¾‹
    examples = load_examples(CONFIG["example_json_file"], CONFIG["image_directory"])
    if not examples:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆç¤ºä¾‹ï¼Œæ— æ³•è¿›è¡Œç¤ºä¾‹å¼•å¯¼ï¼Œç¨‹åºé€€å‡º")
        return
    print(f"âœ… åŠ è½½åˆ°{len(examples)}æ¡æœ‰æ•ˆå‚è€ƒç¤ºä¾‹")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½{CONFIG['model_id']}æ¨¡å‹å’Œåˆ†è¯å™¨...")
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = MistralTokenizer.from_hf_hub(CONFIG["model_id"])
        
        # åŠ è½½æ¨¡å‹
        model = Mistral3ForConditionalGeneration.from_pretrained(
            CONFIG["model_id"], 
            torch_dtype=torch.bfloat16,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨GPU
            low_cpu_mem_usage=True  # å‡å°‘CPUå†…å­˜å ç”¨
        ).eval()
        
        print(f"âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆï¼Œæ¨¡å‹ä½äº: {model.device}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # å¤„ç†æ•°æ®
    print("\nğŸš€ å¼€å§‹å¤„ç†ä¸»æ•°æ®...")
    process_json_data(
        input_json=CONFIG["input_json_file"],
        output_json=CONFIG["output_json_file"],
        system_prompt=system_prompt,
        image_dir=CONFIG["image_directory"],
        model=model,
        tokenizer=tokenizer,
        examples=examples
    )

if __name__ == "__main__":
    main()
